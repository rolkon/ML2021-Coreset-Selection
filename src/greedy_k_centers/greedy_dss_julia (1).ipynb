{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "greedy_dss_julia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_tLuvKDficY"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.utils.data as torch_data\r\n",
        "from torch.optim.lr_scheduler import _LRScheduler\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "import time\r\n",
        "\r\n",
        "import random\r\n",
        "random.seed(42)\r\n",
        "torch.manual_seed(42)\r\n",
        "np.random.seed(42)\r\n",
        "torch.cuda.manual_seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni-NHWHtjTuj"
      },
      "source": [
        "class ShallowNet(torch.nn.Module):\r\n",
        "    def __init__(self, input_dim, num_class):\r\n",
        "        super(ShallowNet, self).__init__()\r\n",
        "        self.fc1 = torch.nn.Linear(input_dim, 100)\r\n",
        "        self.fc2 = torch.nn.Linear(100, num_class)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.fc1(x)\r\n",
        "        out = torch.nn.ReLU()(out)\r\n",
        "        out = self.fc2(out)\r\n",
        "        return out "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTPXlo19umG-"
      },
      "source": [
        "class Dna(torch_data.Dataset):\r\n",
        "\r\n",
        "    def __init__(self, X, y):\r\n",
        "        super(Dna, self).__init__()\r\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\r\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "\r\n",
        "        return len(self.X)\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        \r\n",
        "        return self.X[idx], self.y[idx]\r\n",
        "\r\n",
        "\r\n",
        "dna = pd.read_csv(\"dna.csv\")\r\n",
        "dna_target = dna[\"class\"] - 1 # because otherwise we get classes 0,1,2,3\r\n",
        "dna_features = dna.iloc[:, :-1]\r\n",
        "\r\n",
        "train_dna = Dna(np.array(dna_features.iloc[:1401, :]), np.array(dna_target[:1401]))\r\n",
        "val_dna = Dna(np.array(dna_features.iloc[1400:2001, :]), np.array(dna_target[1400:2001]))\r\n",
        "test_dna = Dna(np.array(dna_features.iloc[2000:, :]), np.array(dna_target[2000:]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7boAr1SjUGb"
      },
      "source": [
        "def compute_grad_train(observations, theta, input_dim, n_classes):\r\n",
        "  '''\r\n",
        "  observations[0]: features, observations[1]: target\r\n",
        "  '''\r\n",
        "  feats = observations[0].reshape(1, input_dim)\r\n",
        "  target = observations[1].reshape(1,)\r\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "  net = ShallowNet(input_dim, n_classes)\r\n",
        "  criterion = torch.nn.CrossEntropyLoss()\r\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\r\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\r\n",
        "\r\n",
        "  # do the forward pass\r\n",
        "  epochs=200\r\n",
        "  for epoch in range(1, epochs+1):\r\n",
        "    net.train()\r\n",
        "    optimizer.zero_grad()\r\n",
        "    x = net(feats)\r\n",
        "    train_loss = criterion(x, target)\r\n",
        "    train_grad = torch.autograd.grad(train_loss, net.fc2.weight, retain_graph=True)[0]\r\n",
        "    train_loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  return train_grad.reshape(1, n_classes*100), net"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtI-uUrlo3L1"
      },
      "source": [
        "def compute_grad_val(net, observations, theta, input_dim, n_classes):\r\n",
        "  '''\r\n",
        "  observations[0]: features, observations[1]: target\r\n",
        "  '''\r\n",
        "  feats = observations[:][0]\r\n",
        "  target = observations[:][1]\r\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "  net = net\r\n",
        "  criterion = torch.nn.CrossEntropyLoss()\r\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\r\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\r\n",
        "  # do the forward pass\r\n",
        "  epochs=200\r\n",
        "  for epoch in range(1, epochs+1):\r\n",
        "    net.eval()\r\n",
        "    optimizer.zero_grad()\r\n",
        "    x = net(feats)\r\n",
        "    val_loss = criterion(x, target)\r\n",
        "    val_grad = torch.autograd.grad(val_loss, net.fc2.weight, retain_graph=True)[0]\r\n",
        "    val_loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  return val_grad.reshape(1, n_classes*100), val_loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NEhqHQHNwBn"
      },
      "source": [
        "def GreedyDSS(U, Val, theta_prev, eta, k, r, lambd, R, sel):\r\n",
        "  '''\r\n",
        "    Implementation of GreedyDSS (Algorithm 2) from GLISTER paper\r\n",
        "\r\n",
        "    Attributes:\r\n",
        "    ---\r\n",
        "    U: torch.tensor\r\n",
        "      Training data.\r\n",
        "    Val: torch.tensor\r\n",
        "      Validation data.\r\n",
        "    theta_0: torch.tensor\r\n",
        "      Model parameters initialization.\r\n",
        "    eta: float\r\n",
        "      Learning rate.\r\n",
        "    k: int\r\n",
        "      Number of point for which the model would be trained.\r\n",
        "    r: int\r\n",
        "      Number of Taylor approximations.\r\n",
        "    lambd: float\r\n",
        "      Regularization coefficient.\r\n",
        "    R: function\r\n",
        "      Regularization function.\r\n",
        "    sel: str\r\n",
        "      Selection method.\r\n",
        "\r\n",
        "    Returns\r\n",
        "    ---\r\n",
        "    S: ndarray\r\n",
        "      Coreset.\r\n",
        "  '''\r\n",
        "  eps=800\r\n",
        "  t = 0\r\n",
        "  S = U[[np.random.randint(len(U)), np.random.randint(len(U))]] # choose one random observation of training set\r\n",
        "  theta = theta_prev\r\n",
        "\r\n",
        "  while t < r:\r\n",
        "    if sel == \"naive_greedy\":\r\n",
        "      V = U\r\n",
        "    elif sel == \"stochastic_greedy\":\r\n",
        "      V = random.sample(list(U), round((len(U)/r) * (1/np.log(eps))))\r\n",
        "\r\n",
        "    g_hats = np.array([])\r\n",
        "    for e in V:\r\n",
        "      grad_train, net = compute_grad_train(e, theta, input_dim, n_classes)\r\n",
        "      theta_t_e = theta + eta *  grad_train\r\n",
        "      grads_s = np.array([])\r\n",
        "      for i, j in zip(S[0], S[1]):\r\n",
        "        grad_s, _ = compute_grad_train((i, j), theta, input_dim, n_classes)\r\n",
        "        grads_s = np.append(grads_s, grad_s)\r\n",
        "      grads_s = np.array(grads_s).reshape(grads_s.shape[0]//(n_classes*100), n_classes*100)\r\n",
        "      theta_s = theta + eta * torch.Tensor(np.sum(grads_s, axis=0))\r\n",
        "\r\n",
        "      grad_val, val_loss = compute_grad_val(net, Val, theta_s, input_dim, n_classes)\r\n",
        "      g_hats = np.append(g_hats, val_loss.detach().numpy() + eta * torch.matmul(grad_train, grad_val.T).detach().numpy()[0][0]) + lambd*R# * R(torch.cat(e, S))) # g hats is np.array # the largest values\r\n",
        "    \r\n",
        "    g_hats = np.array(g_hats)\r\n",
        "    \r\n",
        "    best_indices = np.argpartition(np.array(g_hats), -round(k/r))[-round(k/r):]\r\n",
        "\r\n",
        "    \r\n",
        "    S_t = (torch.clone(S[:][0]), torch.clone(S[:][1]))\r\n",
        "    \r\n",
        "    for i in best_indices:\r\n",
        "      if sel == \"stochastic_greedy\":\r\n",
        "        S_t = (torch.vstack((S_t[:][0], V[:][i][0])), torch.hstack((S_t[:][1], V[:][i][1])))\r\n",
        "      else:\r\n",
        "        S_t = (torch.vstack((S_t[:][0], V[:][0][i])), torch.hstack((S_t[:][1], V[:][1][i])))\r\n",
        "\r\n",
        "    S = S_t\r\n",
        "\r\n",
        "    for elem in S_t:\r\n",
        "      if elem in U:\r\n",
        "        rowindex = numpy.where(U==elem)[0][0] # index of the row\r\n",
        "        U = np.delete(U, rowindex, 0)\r\n",
        "\r\n",
        "    grads_theta = torch.zeros(n_classes*100).reshape(1, n_classes*100)\r\n",
        "    for i, j in zip(S_t[0], S_t[1]):\r\n",
        "      grad, _ = compute_grad_train((i, j), theta, input_dim, n_classes)\r\n",
        "\r\n",
        "      grads_theta += grad\r\n",
        "\r\n",
        "    theta = theta + grads_theta\r\n",
        "\r\n",
        "    t += 1\r\n",
        "\r\n",
        "  return S\r\n",
        "    \r\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DexaJGHce4Yr"
      },
      "source": [
        "U = train_dna\r\n",
        "V = val_dna\r\n",
        "theta_prev = torch.rand(1, 300)\r\n",
        "eta = 0.05\r\n",
        "k = 50\r\n",
        "r = 2\r\n",
        "\r\n",
        "lambd = 0\r\n",
        "sel = \"stochastic_greedy\"\r\n",
        "input_dim = 180\r\n",
        "n_classes = 3\r\n",
        "\r\n",
        "start = time.perf_counter()\r\n",
        "subset = GreedyDSS(U=U, Val=V, theta_prev=theta_prev, eta=eta, k=k, r=r, lambd=0, R=0, sel=sel)\r\n",
        "end = time.perf_counter()\r\n",
        "print(\"time elapsed: \", end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CxWbu-C8TDT",
        "outputId": "438ecd1c-4532-474e-a247-06a46dc51a7f"
      },
      "source": [
        "subset"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "          1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
              "          1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "          0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "          0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
              "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
              "          0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
              "          0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "          0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
              "         [1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
              "          0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
              "          0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
              "          0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
              "          0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "          0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "          0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "          1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
              "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.]]),\n",
              " tensor([0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}