{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import random_split, SequentialSampler, BatchSampler, RandomSampler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_dss(X_train, X_val, theta, eta, k, r, lambd, R, sel, loss, loss_grad):\n",
    "    '''\n",
    "    Attributes:\n",
    "    -----\n",
    "    \n",
    "    X_train: ndarray\n",
    "        Training set (U)\n",
    "    X_val: ndarray\n",
    "        Validation set (V)\n",
    "    theta: ndarray\n",
    "        Vector of current parameters\n",
    "    eta: int\n",
    "        Learning rate\n",
    "    k: ??\n",
    "        Budget\n",
    "    r: int\n",
    "        Number of times to do Taylor-series approximation\n",
    "    lambd: int\n",
    "        Regularization coefficient\n",
    "    R: function\n",
    "        Regularization function\n",
    "    sel: String\n",
    "        Selection method type, one of {'naive_greedy', 'stochastic_greedy'}\n",
    "    loss: function\n",
    "        Function which accepts training subsample and a vector of parameters and returns value of LLT\n",
    "    loss_grad: function\n",
    "        Function which accepts training subsample and a vector of parameters and returns gradient of LLT\n",
    "    '''\n",
    "    \n",
    "    S = None\n",
    "    U = X_train\n",
    "    eps = 1e-5\n",
    "    \n",
    "    for i in range(r):\n",
    "        if sel == 'naive_greedy':\n",
    "            V = U\n",
    "        else if sel == 'stochastic_greedy':\n",
    "            inds = np.random.choice(np.arange(U.shape[0]), \n",
    "                                    size = U.shape[0] / (r * np.log(eps)), \n",
    "                                    replace = False)\n",
    "            V = U[inds, :]\n",
    "    \n",
    "        theta = theta + eta * loss_grad(V, theta)\n",
    "        G = G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "class SetFunctionLoader_2(object):\n",
    "    def __init__(self, trainset, x_val, y_val, model, loss_criterion,\n",
    "                 loss_nored, eta, device, num_classes, batch_size):\n",
    "        self.trainset = trainset  # assume its a sequential loader.\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.model = model\n",
    "        self.loss = loss_criterion  # Make sure it has reduction='none' instead of default\n",
    "        self.loss_nored = loss_nored\n",
    "        self.eta = eta  # step size for the one step gradient update\n",
    "        # self.opt = optimizer\n",
    "        self.device = device\n",
    "        self.N_trn = len(trainset)\n",
    "        self.grads_per_elem = None\n",
    "        self.theta_init = None\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _compute_per_element_grads(self, theta_init):\n",
    "        self.model.load_state_dict(theta_init)\n",
    "        batch_wise_indices = np.array(\n",
    "            [list(BatchSampler(SequentialSampler(self.remainList), self.batch_size, drop_last=False))][0])\n",
    "        cnt = 0\n",
    "        for batch_idx in batch_wise_indices:\n",
    "            inputs = torch.cat(\n",
    "                [self.trainset[x][0].view(-1, 3, self.trainset[x][0].shape[1], self.trainset[x][0].shape[2]) for x in\n",
    "                 batch_idx], dim=0).type(torch.float)\n",
    "            targets = torch.tensor([self.trainset[x][1] for x in batch_idx])\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device, non_blocking=True)\n",
    "            if cnt == 0:\n",
    "                with torch.no_grad():\n",
    "                    data = F.softmax(self.model(inputs), dim=1)\n",
    "                tmp_tensor = torch.zeros(len(inputs), self.num_classes).to(self.device)\n",
    "                tmp_tensor.scatter_(1, targets.view(-1, 1), 1)\n",
    "                outputs = tmp_tensor\n",
    "                cnt = cnt + 1\n",
    "            else:\n",
    "                cnt = cnt + 1\n",
    "                with torch.no_grad():\n",
    "                    data = torch.cat((data, F.softmax(self.model(inputs), dim=1)), dim=0)\n",
    "                tmp_tensor = torch.zeros(len(inputs), self.num_classes).to(self.device)\n",
    "                tmp_tensor.scatter_(1, targets.view(-1, 1), 1)\n",
    "                outputs = torch.cat((outputs, tmp_tensor), dim=0)\n",
    "        grads_vec = data - outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Per Element Gradient Computation is Completed\")\n",
    "        self.N_trn = len(grads_vec)\n",
    "        self.grads_per_elem = grads_vec\n",
    "\n",
    "    def _update_grads_val(self, theta_init, grads_currX=None, first_init=False):\n",
    "        self.model.load_state_dict(theta_init)\n",
    "        self.model.zero_grad()\n",
    "        if first_init:\n",
    "            with torch.no_grad():\n",
    "                scores = F.softmax(self.model(self.x_val), dim=1)\n",
    "                one_hot_label = torch.zeros(len(self.y_val), self.num_classes).to(self.device)\n",
    "                one_hot_label.scatter_(1, self.y_val.view(-1, 1), 1)\n",
    "                grads = scores - one_hot_label\n",
    "        # populate the gradients in model params based on loss.\n",
    "        elif grads_currX is not None:\n",
    "            # update params:\n",
    "            with torch.no_grad():\n",
    "                params = [param for param in self.model.parameters()]\n",
    "                params[-1].data.sub_(self.eta * grads_currX)\n",
    "                scores = F.softmax(self.model(self.x_val), dim=1)\n",
    "                one_hot_label = torch.zeros(len(self.y_val), self.num_classes).to(self.device)\n",
    "                one_hot_label.scatter_(1, self.y_val.view(-1, 1), 1)\n",
    "                grads = scores - one_hot_label\n",
    "        self.grads_val_curr = grads.mean(dim=0)  # reset parm.grads to zero!\n",
    "\n",
    "    def eval_taylor(self, grads_elem, theta_init):\n",
    "        grads_val = self.grads_val_curr\n",
    "        dot_prod = 0\n",
    "        self.model.load_state_dict(theta_init)\n",
    "        with torch.no_grad():\n",
    "            params = [param for param in self.model.parameters()]\n",
    "            dot_prod += torch.sum(grads_val[0] * (params[-1].data - self.eta * grads_elem[0]))\n",
    "        return dot_prod.data\n",
    "\n",
    "    def eval_taylor_modular(self, grads, theta_init):\n",
    "        grads_val = self.grads_val_curr\n",
    "        self.model.load_state_dict(theta_init)\n",
    "        with torch.no_grad():\n",
    "            grads_tensor = torch.cat(grads, dim=0)\n",
    "            param_update = self.eta * grads_tensor\n",
    "            gains = torch.matmul(param_update, grads_val)\n",
    "        return gains\n",
    "\n",
    "    # Updates gradients of set X + element (basically adding element to X)\n",
    "    # Note that it modifies the inpute vector! Also grads_X is a list! grad_e is a tuple!\n",
    "    def _update_gradients_subset(self, grads_X, element):\n",
    "        grads_e = self.grads_per_elem[element]\n",
    "        grads_X += grads_e\n",
    "\n",
    "    # Same as before i.e full batch case! No use of dataloaders here!\n",
    "    # Everything is abstracted away in eval call\n",
    "    def naive_greedy_max(self, budget,remainList,theta_init): # REMAIN LIST IS NOT USED!!!!\n",
    "        self.remainList = remainList\n",
    "        start_time = time.time()\n",
    "        self._compute_per_element_grads(theta_init)\n",
    "        end_time = time.time()\n",
    "        print(\"Per Element gradient computation time is: \", end_time - start_time)\n",
    "        start_time = time.time()\n",
    "        self._update_grads_val(theta_init, first_init=True)\n",
    "        end_time = time.time()\n",
    "        print(\"Updated validation set gradient computation time is: \", end_time - start_time)\n",
    "        # Dont need the trainloader here!! Same as full batch version!\n",
    "        numSelected = 0\n",
    "        grads_currX = []  # basically stores grads_X for the current greedy set X\n",
    "        greedySet = list()\n",
    "        remainSet = list(range(self.N_trn))\n",
    "        t_ng_start = time.time()  # naive greedy start time\n",
    "        subset_size = int((len(self.grads_per_elem) / budget) * math.log(100))\n",
    "        while (numSelected < budget):\n",
    "            # Try Using a List comprehension here!\n",
    "            t_one_elem = time.time()\n",
    "            subset_selected = list(np.random.choice(np.array(list(remainSet)), size=subset_size, replace=False))\n",
    "            rem_grads = [self.grads_per_elem[x].view(1, self.grads_per_elem[0].shape[0]) for x in subset_selected]\n",
    "            gains = self.eval_taylor_modular(rem_grads, theta_init)\n",
    "            # Update the greedy set and remaining set\n",
    "            bestId = subset_selected[torch.argmax(gains)]\n",
    "            greedySet.append(bestId)\n",
    "            remainSet.remove(bestId)\n",
    "            # Update info in grads_currX using element=bestId\n",
    "            if numSelected > 0:\n",
    "                self._update_gradients_subset(grads_currX, bestId)\n",
    "            else:  # If 1st selection, then just set it to bestId grads\n",
    "                grads_currX = self.grads_per_elem[bestId]  # Making it a list so that is mutable!\n",
    "            # Update the grads_val_current using current greedySet grads\n",
    "            self._update_grads_val(theta_init, grads_currX)\n",
    "            if numSelected % 1000 == 0:\n",
    "                # Printing bestGain and Selection time for 1 element.\n",
    "                print(\"numSelected:\", numSelected, \"Time for 1:\", time.time() - t_one_elem)\n",
    "            numSelected += 1\n",
    "        print(\"Naive greedy total time:\", time.time() - t_ng_start)\n",
    "        return list(greedySet), grads_currX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy, datetime, time\n",
    "\n",
    "def prepare_data(X, y, test_size, val_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = val_size)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_dss(X, y, num_cls, train_batch_size,\n",
    "               theta, learning_rate, bud, num_epochs, lam, R, sel):\n",
    "    '''\n",
    "    Attributes:\n",
    "    -----\n",
    "    \n",
    "    X_train: ndarray\n",
    "        Training set (U)\n",
    "    X_val: ndarray\n",
    "        Validation set (V)\n",
    "    theta: ndarray\n",
    "        Vector of current parameters\n",
    "    learning_rate: int\n",
    "        Learning rate (eta)\n",
    "    bud: int\n",
    "        Budget (k)\n",
    "    num_epochs: int\n",
    "        Number of times to do Taylor-series approximation (r)\n",
    "    lam: int\n",
    "        Regularization coefficient\n",
    "    R: function\n",
    "        Regularization function\n",
    "    sel: String\n",
    "        Selection method type, one of {'naive_greedy', 'stochastic_greedy'}\n",
    "    loss: function\n",
    "        Function which accepts training subsample and a vector of parameters and returns value of LLT\n",
    "    loss_grad: function\n",
    "        Function which accepts training subsample and a vector of parameters and returns gradient of LLT\n",
    "    '''\n",
    "    \n",
    "    # Set train-val-test sets parameters\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    # Prepare sample\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = prepare_data(X, y, 0.3, 0.3)\n",
    "    \n",
    "    # Random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Set up the model\n",
    "    model = MnistNet() # Choose the model\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction = 'none')\n",
    "    optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    # TODO: Stochastic greedy vs naive greedy?\n",
    "    idxs = np.arange(X_train.shape[0])\n",
    "    total_idxs = np.arange(X_train.shape[0])\n",
    "    \n",
    "    substrn_losses = np.zeros(num_epochs)\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    \n",
    "    subset_trnloader = torch.utils.data.DataLoader(X_train, batch_size = train_batch_size, \n",
    "                                                   shuffle = False,\n",
    "                                                   sampler = SubsetRandomSampler(idxs),\n",
    "                                                   pin_memory = True)\n",
    "    \n",
    "    setf_model = SetFunctionLoader_2(X_train, X_val, y_val, model, criterion,\n",
    "                             criterion_nored, learning_rate, device, num_cls, 1000)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        actual_idxs = idxs\n",
    "        batch_wise_indices = [actual_idxs[x] for x in list(BatchSampler(RandomSampler(actual_idxs), train_batch_size, drop_last=False))]\n",
    "        subtrn_loss = 0\n",
    "        for batch_idx in batch_wise_indices:\n",
    "            inp = np.expand_dims(X[batch_idx, :], 1)\n",
    "            inp = inp.reshape(len(batch_idx), 1, 8, 8) # HARD_CODED\n",
    "            inputs = torch.from_numpy(inp).type(torch.float)\n",
    "            targets = torch.from_numpy(y[batch_idx]).type(torch.LongTensor)\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            subtrn_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = 0\n",
    "        full_trn_loss = 0\n",
    "\n",
    "        with torch.no_grad(): # TODO: batches\n",
    "            inp = np.expand_dims(X_val, 1)\n",
    "            inp = inp.reshape(X_val.shape[0], 1, 8, 8) # HARD_CODED\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # TODO: bathces\n",
    "            inp = np.expand_dims(X_train, 1)\n",
    "            inp = inp.reshape(X_train.shape[0], 1, 8, 8) # HARD_CODED\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "\n",
    "        substrn_losses[i] = subtrn_loss\n",
    "        fulltrn_losses[i] = full_trn_loss\n",
    "        val_losses[i] = val_loss\n",
    "        print('Epoch:', i + 1, 'SubsetTrn,FullTrn,ValLoss:', subtrn_loss, full_trn_loss, val_loss)\n",
    "        \n",
    "        cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "        clone_dict = copy.deepcopy(model.state_dict())\n",
    "        print(\"selEpoch: %d, Starting Selection:\" % i, str(datetime.datetime.now()))\n",
    "        subset_idxs, grads_idxs = setf_model.naive_greedy_max(int(bud), clone_dict)\n",
    "        rem_idxs = list(set(total_idxs).difference(set(subset_idxs)))\n",
    "        subset_idxs.extend(list(np.random.choice(rem_idxs, size=int((1 - lam) * bud), replace=False)))\n",
    "        idxs = subset_idxs\n",
    "        \n",
    "        print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "        model.load_state_dict(cached_state_dict)\n",
    "        \n",
    "        ### Change the subset_trnloader according to new found indices: subset_idxs\n",
    "        subset_trnloader = torch.utils.data.DataLoader(trainset, batch_size=trn_batch_size, shuffle=False,\n",
    "                                                       sampler=SubsetRandomSampler(idxs), num_workers=1,\n",
    "                                                       pin_memory=True)\n",
    "            \n",
    "    time = start.elapsed_time(end)/1000\n",
    "    subtrn_loss = 0\n",
    "    subtrn_correct = 0\n",
    "    subtrn_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(subset_trnloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            subtrn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            subtrn_total += targets.size(0)\n",
    "            subtrn_correct += predicted.eq(targets).sum().item()\n",
    "    subtrn_acc = subtrn_correct / subtrn_total\n",
    "\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "    full_trn_acc = full_trn_correct / full_trn_total\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    tst_acc = correct / total\n",
    "\n",
    "    print(\"SelectionRun---------------------------------\")\n",
    "    print(\"Final SubsetTrn and FullTrn Loss:\", subtrn_loss, full_trn_loss)\n",
    "    print(\"Validation Loss and Accuracy:\", val_loss, val_acc)\n",
    "    print(\"Test Data Loss and Accuracy:\", test_loss, tst_acc)\n",
    "    print('-----------------------------------')\n",
    "    return val_acc, tst_acc,  subtrn_acc, full_trn_acc, val_loss, test_loss, subtrn_loss, full_trn_loss, val_losses, substrn_losses, fulltrn_losses, idxs, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()\n",
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 SubsetTrn,FullTrn,ValLoss: 61.68104887008667 2.253173589706421 2.251558303833008\n",
      "selEpoch: 0, Starting Selection: 2021-03-08 10:33:45.150135\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "naive_greedy_max() missing 1 required positional argument: 'theta_init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-369-9c5ebb84829c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgreedy_dss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-366-66ed43fffe5a>\u001b[0m in \u001b[0;36mgreedy_dss\u001b[0;34m(X, y, num_cls, train_batch_size, theta, learning_rate, bud, num_epochs, lam, R, sel)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mclone_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"selEpoch: %d, Starting Selection:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0msubset_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_greedy_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mrem_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0msubset_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrem_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: naive_greedy_max() missing 1 required positional argument: 'theta_init'"
     ]
    }
   ],
   "source": [
    "greedy_dss(X, y, 10, 50, 0, 1, 5, 5, 0.1, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
