# -*- coding: utf-8 -*-
"""cifar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/139-N-Gs-PJ4cT4mmEdmSzQk3Smh7S8Gt
"""

import pandas as pd
import numpy as np
import torch
import torch.utils.data as torch_data
from torch.optim.lr_scheduler import _LRScheduler
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader
import time
from torchvision.datasets import CIFAR10
import torch.nn.functional as F

import torch.nn as nn

import random
random.seed(42)
torch.manual_seed(42)
np.random.seed(42)
torch.cuda.manual_seed(42)

class CifarNet(torch.nn.Module):
    def __init__(self):
        super(CifarNet, self).__init__()
        self.conv1 = nn.Conv2d(3,   64,  3)
        self.conv2 = nn.Conv2d(64,  128, 3)
        self.conv3 = nn.Conv2d(128, 256, 3)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 4 * 4, 128)
        self.fc2 = nn.Linear(128, 256)
        self.fc3 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(-1, 64 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

def compute_grad_train(observations, theta, input_dim, n_classes):
  '''
  observations[0]: features, observations[1]: target
  '''
  feats = observations[0].reshape(1,3,32,32)
  target = torch.tensor(observations[1]).reshape(1,)
  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
  net = CifarNet()
  criterion = torch.nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)
  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)

  # do the forward pass
  epochs=200
  for epoch in range(1, epochs+1):
    net.train()
    optimizer.zero_grad()
    x = net(feats)
    train_loss = criterion(x, target)
    train_grad = torch.autograd.grad(train_loss, net.fc3.weight, retain_graph=True)[0]
    train_loss.backward()
    optimizer.step()

  return train_grad.reshape(1, 2560), net

def compute_grad_val(net, observations, theta, input_dim, n_classes):
  '''
  observations[0]: features, observations[1]: target
  '''
  val_loader = DataLoader(observations, batch_size=256, shuffle=False)
  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
  net = net
  criterion = torch.nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)
  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)
  # do the forward pass
  epochs=20
  for epoch in range(1, epochs+1):
    net.eval()
    for X, target in val_loader:
      X, target = X.to(device), target.to(device)
      optimizer.zero_grad()
      x = net(X)
      val_loss = criterion(x, target)
      val_grad = torch.autograd.grad(val_loss, net.fc3.weight, retain_graph=True)[0]
      val_loss.backward()
      optimizer.step()

  return val_grad.reshape(1, 2560), val_loss

def GreedyDSS(U, Val, theta_prev, eta, k, r, lambd, R, sel):
  '''
    Implementation of GreedyDSS (Algorithm 2) from GLISTER paper

    Attributes:
    ---
    U: torch.tensor
      Training data.
    Val: torch.tensor
      Validation data.
    theta_0: torch.tensor
      Model parameters initialization.
    eta: float
      Learning rate.
    k: int
      Number of point for which the model would be trained.
    r: int
      Number of Taylor approximations.
    lambd: float
      Regularization coefficient.
    R: function
      Regularization function.
    sel: str
      Selection method.

    Returns
    ---
    S: ndarray
      Coreset.
  '''
  eps=1000
  t = 0
  S = [U[np.random.randint(len(U))], U[np.random.randint(len(U))]] # choose one random observation of training set
  theta = theta_prev

  while t < r:
    if sel == "naive_greedy":
      V = U
    elif sel == "stochastic_greedy":
      random_indices = []
      V = random.sample(list(U), round((len(U)/r)*(1/np.log(eps))))


    g_hats = np.array([])

    for e in V:
      grad_train, net = compute_grad_train(e, theta, input_dim, n_classes)
      theta_t_e = theta + eta *  grad_train
      grads_s = np.array([])

      for i in range(len(S)):
        grad_s, _ = compute_grad_train(S[i], theta, input_dim, n_classes)
        grads_s = np.append(grads_s, grad_s)
      grads_s = np.array(grads_s).reshape(grads_s.shape[0]//2560, 2560)
      theta_s = theta + eta * torch.Tensor(np.sum(grads_s, axis=0))

      grad_val, val_loss = compute_grad_val(net, Val, theta_s, input_dim, n_classes)
      g_hats = np.append(g_hats, val_loss.detach().numpy() + eta * torch.matmul(grad_train, grad_val.T).detach().numpy()[0][0]) + lambd*R# * R(torch.cat(e, S))) # g hats is np.array # the largest values
    
    g_hats = np.array(g_hats)
    
    best_indices = np.argpartition(np.array(g_hats), -round(k/r))[-round(k/r):] # S_t: indices in V
    #S_t = (torch.clone(S[:][0]), torch.clone(S[:][1]))
    
    for i in best_indices:
      if sel == "stochastic_greedy":
        #S = (torch.vstack((S[:][0], V[:][i][0])), torch.hstack((S[:][1], V[:][i][1])))
        S.append(V[i])
      else:
        #S = (torch.vstack((S[:][0], V[:][0][i])), torch.hstack((S[:][1], V[:][1][i])))
        S.append(V[i])

    #for elem in [V[i] for i in best_indices]:
    #  rowindex = numpy.where(U==elem)[0][0] # index of the row
    #  U = np.delete(U, rowindex, 0)

    grads_theta = torch.zeros(2560).reshape(1, 2560)
    for i in best_indices:
      grad, _ = compute_grad_train(V[i], theta, input_dim, n_classes)

      grads_theta += grad

    theta = theta + grads_theta

    t += 1

  return S

def glister_online(U, Val, S_0, k, theta_prev, eta, T, L, r, lambd, R, sel):
  '''
  Attributes:
  ---
  U: torch.tensor
    Training data.
  Val: torch.tensor
    Validation data.
  S_0: torch.tensor
    Initial subset.
  k: int
    Size of the initial subset.
  theta_prev: torch.tensor
    Model parameter initialization.
  eta: float
    Learning rate.
  T: int
    Total epochs.
  L: int
    Epoch interval for selection.
  r: int
    Number of Taylor approximations.
  lambd: float
    Regularization coefficient.
  R: function
    Regularization function.
  sel: str
    Selection Method.
  
  Returns:
  ---
  S_T: torch.tensor
    Final subset
  theta_T: torch.tensor
    Parameters.
  '''
  theta = theta_prev
  S_t = S_0

  for t in range(T):
    if t % L == 0:
      print("Progress: {t}/{T}".format(t=t, T=T))
      S_t = GreedyDSS(U=U, Val=Val, theta_prev=theta, eta=eta, k=k, r=r, lambd=0, R=0, sel=sel)

    model = CifarNet()
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)
    
    loader = DataLoader(S_t, batch_size=128, shuffle=True)

    for X, y in loader:
      model.train()
      optimizer.zero_grad()
      x = model(X)
      train_loss = criterion(x, y)
      train_loss.backward()
      optimizer.step()

      if scheduler is not None:
        scheduler.step()
      
    theta = model.fc3.weight.reshape(1, 2560)

  return S_t, model.fc3.weight

def train(epochs, net, criterion, optimizer, train_loader, val_loader, scheduler=None, verbose=True, device='cpu'):
    net.to(device)
    freq = max(epochs//15,1)
 
    for epoch in range(1, epochs+1):
        net.train()

        losses_train = []
        for X, target in train_loader:

            X, target = X.to(device), target.to(device)
            
            optimizer.zero_grad()
            x = net(X)
          
            train_loss = criterion(x, target)
            train_loss.backward()
            optimizer.step()
            losses_train.append(train_loss.item())
            
            
        if scheduler is not None:
            scheduler.step()
        
        if verbose and epoch%freq==0:
            y_pred_val =  []
            y_true_val = []
            net.eval()
            for X, target in val_loader:
                X, target = X.to(device), target.to(device)
                losses_val = []  

                optimizer.zero_grad()
                x = net(X)
                target_hat_val = torch.nn.Softmax(1)(x)

                val_loss = criterion(x, target)
                val_loss.backward()
                optimizer.step()
                losses_val.append(val_loss.item())
                                
                y_pred_val.extend(target_hat_val.argmax(1).tolist())
                y_true_val.extend(target.tolist())

            mean_val = sum(losses_val)/len(losses_val)
            mean_train = sum(losses_train)/len(losses_train)
            
            print('Val epoch {}'.format(epoch), ', Loss : {:.3}'.format(mean_train), ', Accuracy on test: {:.3}'.format(accuracy_score(y_true_val, y_pred_val)))

def experimental_setting(train_data, val_data, test_data, theta_prev, eta, k, r, lambd, R, sel, input_dim, n_classes, T, L, batch_size, network):

  print("Glister online:")
  start = time.perf_counter()
  subset = glister_online(U=train_data, Val=val_data, theta_prev=theta_prev, S_0 = [train_data[np.random.randint(len(train_data))], train_data[np.random.randint(len(train_data))]], eta=eta, k=k, r=r, lambd=lambd, R=R, sel=sel, T=T, L=L)
  print("time elapsed: ", time.perf_counter()-start)

  print("Testing the model:")
  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
  net = CifarNet()
  criterion = torch.nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)
  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)

  train_loader = DataLoader(subset[0], batch_size=128, shuffle=True)
  val_loader = DataLoader(val_data, batch_size=batch_size*2, shuffle=False)
  test_loader = DataLoader(test_data, batch_size=batch_size*2, shuffle=False)

  train(100, net, criterion, optimizer, train_loader, test_loader, scheduler, verbose=True)

  print("Random comparison:")
  random_sample = random.sample(list(train_data), k)
  random_samp_loader = DataLoader(random_sample, batch_size=batch_size, shuffle=True)

  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
  net_2 = CifarNet()
  criterion = torch.nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(net_2.parameters(), lr=1e-3)
  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)
  train(100, net_2, criterion, optimizer, random_samp_loader, test_loader, scheduler, verbose=True)

# ---------- CIFAR data set ------------------------------------------------------------------------------------------------------------------------------
n_classes = 10
input_dim = 32*32*3

from torchvision.datasets import CIFAR10
from torchvision.transforms import ToTensor
from torch.utils.data import random_split
import torchvision.transforms as transforms


transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

dataset = CIFAR10(root='data/', download=True, transform=transform)
test_dataset = CIFAR10(root='data/', train=False, transform=transform)

val_size = 5000
train_size = len(dataset) - val_size
train_ds, val_ds = random_split(dataset, [train_size, val_size])
batch_size=128


print("Approximately 10% of data")
experimental_setting(train_ds, val_ds, test_dataset, torch.randn(1, 2560), 0.05, 5000, 5000, 0, 0, "stochastic_greedy", 32*32*3, 10, 100, 20, 128, CifarNet()) # ~10% of data

print("Approximately 30% of data")
experimental_setting(train_ds, val_ds, test_dataset, torch.randn(1, 2560), 0.05, 15000, 15000, 0, 0, "stochastic_greedy", 32*32*3, 10, 100, 20, 128, CifarNet()) # ~30% of data

print("Approximately 50% of data")
experimental_setting(train_ds, val_ds, test_dataset, torch.randn(1, 2560), 0.05, 25000, 25000, 0, 0, "stochastic_greedy", 32*32*3, 10, 100, 20, 128, CifarNet()) # ~50% of data



