{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "greedy_dss_julia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_tLuvKDficY"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.utils.data as torch_data\r\n",
        "from torch.optim.lr_scheduler import _LRScheduler\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import time\r\n",
        "\r\n",
        "import random\r\n",
        "random.seed(42)\r\n",
        "torch.manual_seed(42)\r\n",
        "np.random.seed(42)\r\n",
        "torch.cuda.manual_seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni-NHWHtjTuj"
      },
      "source": [
        "class ShallowNet(torch.nn.Module):\r\n",
        "    def __init__(self, input_dim, num_class):\r\n",
        "        super(ShallowNet, self).__init__()\r\n",
        "        self.fc1 = torch.nn.Linear(input_dim, 100)\r\n",
        "        self.fc2 = torch.nn.Linear(100, num_class)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.fc1(x)\r\n",
        "        out = torch.nn.ReLU()(out)\r\n",
        "        out = self.fc2(out)\r\n",
        "        return out "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTPXlo19umG-"
      },
      "source": [
        "class Dna(torch_data.Dataset):\r\n",
        "\r\n",
        "    def __init__(self, X, y):\r\n",
        "        super(Dna, self).__init__()\r\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\r\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "\r\n",
        "        return len(self.X)\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        \r\n",
        "        return self.X[idx], self.y[idx]\r\n",
        "\r\n",
        "\r\n",
        "dna = pd.read_csv(\"dna.csv\")\r\n",
        "dna_target = dna[\"class\"] - 1 # because otherwise we get classes 0,1,2,3\r\n",
        "dna_features = dna.iloc[:, :-1]\r\n",
        "\r\n",
        "train_dna = Dna(np.array(dna_features.iloc[:1401, :]), np.array(dna_target[:1401]))\r\n",
        "val_dna = Dna(np.array(dna_features.iloc[1400:2001, :]), np.array(dna_target[1400:2001]))\r\n",
        "test_dna = Dna(np.array(dna_features.iloc[2000:, :]), np.array(dna_target[2000:]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7boAr1SjUGb"
      },
      "source": [
        "def compute_grad_train(observations, theta, input_dim, n_classes):\r\n",
        "  '''\r\n",
        "  observations[0]: features, observations[1]: target\r\n",
        "  '''\r\n",
        "  feats = observations[0].reshape(1, input_dim)\r\n",
        "  target = observations[1].reshape(1,)\r\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "  net = ShallowNet(input_dim, n_classes)\r\n",
        "  criterion = torch.nn.CrossEntropyLoss()\r\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\r\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\r\n",
        "\r\n",
        "  # do the forward pass\r\n",
        "  epochs=200\r\n",
        "  for epoch in range(1, epochs+1):\r\n",
        "    net.train()\r\n",
        "    optimizer.zero_grad()\r\n",
        "    x = net(feats)\r\n",
        "    train_loss = criterion(x, target)\r\n",
        "    train_grad = torch.autograd.grad(train_loss, net.fc2.weight, retain_graph=True)[0]\r\n",
        "    train_loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  return train_grad.reshape(1, n_classes*100), net"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtI-uUrlo3L1"
      },
      "source": [
        "def compute_grad_val(net, observations, theta, input_dim, n_classes):\r\n",
        "  '''\r\n",
        "  observations[0]: features, observations[1]: target\r\n",
        "  '''\r\n",
        "  feats = observations[:][0]\r\n",
        "  target = observations[:][1]\r\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "  net = net\r\n",
        "  criterion = torch.nn.CrossEntropyLoss()\r\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\r\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\r\n",
        "  # do the forward pass\r\n",
        "  epochs=200\r\n",
        "  for epoch in range(1, epochs+1):\r\n",
        "    net.eval()\r\n",
        "    optimizer.zero_grad()\r\n",
        "    x = net(feats)\r\n",
        "    val_loss = criterion(x, target)\r\n",
        "    val_grad = torch.autograd.grad(val_loss, net.fc2.weight, retain_graph=True)[0]\r\n",
        "    val_loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  return val_grad.reshape(1, n_classes*100), val_loss"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NEhqHQHNwBn"
      },
      "source": [
        "def GreedyDSS(U, Val, theta_prev, eta, k, r, lambd, R, sel):\r\n",
        "  '''\r\n",
        "    Implementation of GreedyDSS (Algorithm 2) from GLISTER paper\r\n",
        "\r\n",
        "    Attributes:\r\n",
        "    ---\r\n",
        "    U: torch.tensor\r\n",
        "      Training data.\r\n",
        "    Val: torch.tensor\r\n",
        "      Validation data.\r\n",
        "    theta_0: torch.tensor\r\n",
        "      Model parameters initialization.\r\n",
        "    eta: float\r\n",
        "      Learning rate.\r\n",
        "    k: int\r\n",
        "      Number of point for which the model would be trained.\r\n",
        "    r: int\r\n",
        "      Number of Taylor approximations.\r\n",
        "    lambd: float\r\n",
        "      Regularization coefficient.\r\n",
        "    R: function\r\n",
        "      Regularization function.\r\n",
        "    sel: str\r\n",
        "      Selection method.\r\n",
        "\r\n",
        "    Returns\r\n",
        "    ---\r\n",
        "    S: ndarray\r\n",
        "      Coreset.\r\n",
        "  '''\r\n",
        "  eps=800\r\n",
        "  t = 0\r\n",
        "  S = U[[np.random.randint(len(U)), np.random.randint(len(U))]] # choose one random observation of training set\r\n",
        "  theta = theta_prev\r\n",
        "\r\n",
        "  while t < r:\r\n",
        "    if sel == \"naive_greedy\":\r\n",
        "      V = U\r\n",
        "    elif sel == \"stochastic_greedy\":\r\n",
        "      V = random.sample(list(U), round((len(U)/r) * (1/np.log(eps))))\r\n",
        "\r\n",
        "    g_hats = np.array([])\r\n",
        "    for e in V:\r\n",
        "      grad_train, net = compute_grad_train(e, theta, input_dim, n_classes)\r\n",
        "      theta_t_e = theta + eta *  grad_train\r\n",
        "      grads_s = np.array([])\r\n",
        "      for i, j in zip(S[0], S[1]):\r\n",
        "        grad_s, _ = compute_grad_train((i, j), theta, input_dim, n_classes)\r\n",
        "        grads_s = np.append(grads_s, grad_s)\r\n",
        "      grads_s = np.array(grads_s).reshape(grads_s.shape[0]//(n_classes*100), n_classes*100)\r\n",
        "      theta_s = theta + eta * torch.Tensor(np.sum(grads_s, axis=0))\r\n",
        "\r\n",
        "      grad_val, val_loss = compute_grad_val(net, Val, theta_s, input_dim, n_classes)\r\n",
        "      g_hats = np.append(g_hats, val_loss.detach().numpy() + eta * torch.matmul(grad_train, grad_val.T).detach().numpy()[0][0]) + lambd*R# * R(torch.cat(e, S))) # g hats is np.array # the largest values\r\n",
        "    \r\n",
        "    g_hats = np.array(g_hats)\r\n",
        "    \r\n",
        "    best_indices = np.argpartition(np.array(g_hats), -round(k/r))[-round(k/r):]\r\n",
        "\r\n",
        "    \r\n",
        "    S_t = (torch.clone(S[:][0]), torch.clone(S[:][1]))\r\n",
        "    \r\n",
        "    for i in best_indices:\r\n",
        "      if sel == \"stochastic_greedy\":\r\n",
        "        S_t = (torch.vstack((S_t[:][0], V[:][i][0])), torch.hstack((S_t[:][1], V[:][i][1])))\r\n",
        "      else:\r\n",
        "        S_t = (torch.vstack((S_t[:][0], V[:][0][i])), torch.hstack((S_t[:][1], V[:][1][i])))\r\n",
        "\r\n",
        "    S = S_t\r\n",
        "\r\n",
        "    for elem in S_t:\r\n",
        "      if elem in U:\r\n",
        "        rowindex = numpy.where(U==elem)[0][0] # index of the row\r\n",
        "        U = np.delete(U, rowindex, 0)\r\n",
        "\r\n",
        "    grads_theta = torch.zeros(n_classes*100).reshape(1, n_classes*100)\r\n",
        "    for i, j in zip(S_t[0], S_t[1]):\r\n",
        "      grad, _ = compute_grad_train((i, j), theta, input_dim, n_classes)\r\n",
        "\r\n",
        "      grads_theta += grad\r\n",
        "\r\n",
        "    theta = theta + grads_theta\r\n",
        "\r\n",
        "    t += 1\r\n",
        "\r\n",
        "  return S\r\n",
        "    \r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYE6VdlGuqaW"
      },
      "source": [
        "def glister_online(U, Val, S_0, k, theta_prev, eta, T, L, r, lambd, R, sel):\r\n",
        "  '''\r\n",
        "  Attributes:\r\n",
        "  ---\r\n",
        "  U: torch.tensor\r\n",
        "    Training data.\r\n",
        "  Val: torch.tensor\r\n",
        "    Validation data.\r\n",
        "  S_0: torch.tensor\r\n",
        "    Initial subset.\r\n",
        "  k: int\r\n",
        "    Size of the initial subset.\r\n",
        "  theta_prev: torch.tensor\r\n",
        "    Model parameter initialization.\r\n",
        "  eta: float\r\n",
        "    Learning rate.\r\n",
        "  T: int\r\n",
        "    Total epochs.\r\n",
        "  L: int\r\n",
        "    Epoch interval for selection.\r\n",
        "  r: int\r\n",
        "    Number of Taylor approximations.\r\n",
        "  lambd: float\r\n",
        "    Regularization coefficient.\r\n",
        "  R: function\r\n",
        "    Regularization function.\r\n",
        "  sel: str\r\n",
        "    Selection Method.\r\n",
        "  \r\n",
        "  Returns:\r\n",
        "  ---\r\n",
        "  S_T: torch.tensor\r\n",
        "    Final subset\r\n",
        "  theta_T: torch.tensor\r\n",
        "    Parameters.\r\n",
        "  '''\r\n",
        "  theta = theta_prev\r\n",
        "  S_t = S_0\r\n",
        "\r\n",
        "  for t in range(T):\r\n",
        "    print(\"Epoch: \", t)\r\n",
        "    if t % L == 0:\r\n",
        "      S_t = GreedyDSS(U=U, Val=V, theta_prev=theta, eta=eta, k=k, r=r, lambd=0, R=0, sel=sel)\r\n",
        "\r\n",
        "    model = ShallowNet(input_dim, n_classes)\r\n",
        "    criterion = torch.nn.CrossEntropyLoss()\r\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\r\n",
        "    \r\n",
        "    loader = DataLoader(Dna(S_t[:][0], S_t[:][1]), batch_size=10, shuffle=True)\r\n",
        "\r\n",
        "    for X, y in loader:\r\n",
        "      model.train()\r\n",
        "      optimizer.zero_grad()\r\n",
        "      x = model(S_t[:][0])\r\n",
        "      train_loss = criterion(x, S_t[:][1])\r\n",
        "      train_loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "      if scheduler is not None:\r\n",
        "        scheduler.step()\r\n",
        "      \r\n",
        "    theta = model.fc2.weight.reshape(1, n_classes*100)\r\n",
        "\r\n",
        "  return S_t, model.fc2.weight\r\n",
        "\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DexaJGHce4Yr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b797bfba-98c4-4619-a232-40affb31c14b"
      },
      "source": [
        "U = train_dna\r\n",
        "V = val_dna\r\n",
        "theta_prev = torch.rand(1, 300)\r\n",
        "eta = 0.05\r\n",
        "k = 5\r\n",
        "r = 5\r\n",
        "lambd = 0\r\n",
        "sel = \"stochastic_greedy\"\r\n",
        "input_dim = 180\r\n",
        "n_classes = 3\r\n",
        "\r\n",
        "start = time.perf_counter()\r\n",
        "subset = glister_online(U=U, Val=V, theta_prev=theta_prev, S_0 = U[[np.random.randint(len(U)), np.random.randint(len(U))]], eta=eta, k=k, r=r, lambd=0, R=0, sel=sel, T=200, L=20)\r\n",
        "end = time.perf_counter()\r\n",
        "print(\"time elapsed: \", end-start)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "Epoch:  2\n",
            "Epoch:  3\n",
            "Epoch:  4\n",
            "Epoch:  5\n",
            "Epoch:  6\n",
            "Epoch:  7\n",
            "Epoch:  8\n",
            "Epoch:  9\n",
            "Epoch:  10\n",
            "Epoch:  11\n",
            "Epoch:  12\n",
            "Epoch:  13\n",
            "Epoch:  14\n",
            "Epoch:  15\n",
            "Epoch:  16\n",
            "Epoch:  17\n",
            "Epoch:  18\n",
            "Epoch:  19\n",
            "Epoch:  20\n",
            "Epoch:  21\n",
            "Epoch:  22\n",
            "Epoch:  23\n",
            "Epoch:  24\n",
            "Epoch:  25\n",
            "Epoch:  26\n",
            "Epoch:  27\n",
            "Epoch:  28\n",
            "Epoch:  29\n",
            "Epoch:  30\n",
            "Epoch:  31\n",
            "Epoch:  32\n",
            "Epoch:  33\n",
            "Epoch:  34\n",
            "Epoch:  35\n",
            "Epoch:  36\n",
            "Epoch:  37\n",
            "Epoch:  38\n",
            "Epoch:  39\n",
            "Epoch:  40\n",
            "Epoch:  41\n",
            "Epoch:  42\n",
            "Epoch:  43\n",
            "Epoch:  44\n",
            "Epoch:  45\n",
            "Epoch:  46\n",
            "Epoch:  47\n",
            "Epoch:  48\n",
            "Epoch:  49\n",
            "Epoch:  50\n",
            "Epoch:  51\n",
            "Epoch:  52\n",
            "Epoch:  53\n",
            "Epoch:  54\n",
            "Epoch:  55\n",
            "Epoch:  56\n",
            "Epoch:  57\n",
            "Epoch:  58\n",
            "Epoch:  59\n",
            "Epoch:  60\n",
            "Epoch:  61\n",
            "Epoch:  62\n",
            "Epoch:  63\n",
            "Epoch:  64\n",
            "Epoch:  65\n",
            "Epoch:  66\n",
            "Epoch:  67\n",
            "Epoch:  68\n",
            "Epoch:  69\n",
            "Epoch:  70\n",
            "Epoch:  71\n",
            "Epoch:  72\n",
            "Epoch:  73\n",
            "Epoch:  74\n",
            "Epoch:  75\n",
            "Epoch:  76\n",
            "Epoch:  77\n",
            "Epoch:  78\n",
            "Epoch:  79\n",
            "Epoch:  80\n",
            "Epoch:  81\n",
            "Epoch:  82\n",
            "Epoch:  83\n",
            "Epoch:  84\n",
            "Epoch:  85\n",
            "Epoch:  86\n",
            "Epoch:  87\n",
            "Epoch:  88\n",
            "Epoch:  89\n",
            "Epoch:  90\n",
            "Epoch:  91\n",
            "Epoch:  92\n",
            "Epoch:  93\n",
            "Epoch:  94\n",
            "Epoch:  95\n",
            "Epoch:  96\n",
            "Epoch:  97\n",
            "Epoch:  98\n",
            "Epoch:  99\n",
            "Epoch:  100\n",
            "Epoch:  101\n",
            "Epoch:  102\n",
            "Epoch:  103\n",
            "Epoch:  104\n",
            "Epoch:  105\n",
            "Epoch:  106\n",
            "Epoch:  107\n",
            "Epoch:  108\n",
            "Epoch:  109\n",
            "Epoch:  110\n",
            "Epoch:  111\n",
            "Epoch:  112\n",
            "Epoch:  113\n",
            "Epoch:  114\n",
            "Epoch:  115\n",
            "Epoch:  116\n",
            "Epoch:  117\n",
            "Epoch:  118\n",
            "Epoch:  119\n",
            "Epoch:  120\n",
            "Epoch:  121\n",
            "Epoch:  122\n",
            "Epoch:  123\n",
            "Epoch:  124\n",
            "Epoch:  125\n",
            "Epoch:  126\n",
            "Epoch:  127\n",
            "Epoch:  128\n",
            "Epoch:  129\n",
            "Epoch:  130\n",
            "Epoch:  131\n",
            "Epoch:  132\n",
            "Epoch:  133\n",
            "Epoch:  134\n",
            "Epoch:  135\n",
            "Epoch:  136\n",
            "Epoch:  137\n",
            "Epoch:  138\n",
            "Epoch:  139\n",
            "Epoch:  140\n",
            "Epoch:  141\n",
            "Epoch:  142\n",
            "Epoch:  143\n",
            "Epoch:  144\n",
            "Epoch:  145\n",
            "Epoch:  146\n",
            "Epoch:  147\n",
            "Epoch:  148\n",
            "Epoch:  149\n",
            "Epoch:  150\n",
            "Epoch:  151\n",
            "Epoch:  152\n",
            "Epoch:  153\n",
            "Epoch:  154\n",
            "Epoch:  155\n",
            "Epoch:  156\n",
            "Epoch:  157\n",
            "Epoch:  158\n",
            "Epoch:  159\n",
            "Epoch:  160\n",
            "Epoch:  161\n",
            "Epoch:  162\n",
            "Epoch:  163\n",
            "Epoch:  164\n",
            "Epoch:  165\n",
            "Epoch:  166\n",
            "Epoch:  167\n",
            "Epoch:  168\n",
            "Epoch:  169\n",
            "Epoch:  170\n",
            "Epoch:  171\n",
            "Epoch:  172\n",
            "Epoch:  173\n",
            "Epoch:  174\n",
            "Epoch:  175\n",
            "Epoch:  176\n",
            "Epoch:  177\n",
            "Epoch:  178\n",
            "Epoch:  179\n",
            "Epoch:  180\n",
            "Epoch:  181\n",
            "Epoch:  182\n",
            "Epoch:  183\n",
            "Epoch:  184\n",
            "Epoch:  185\n",
            "Epoch:  186\n",
            "Epoch:  187\n",
            "Epoch:  188\n",
            "Epoch:  189\n",
            "Epoch:  190\n",
            "Epoch:  191\n",
            "Epoch:  192\n",
            "Epoch:  193\n",
            "Epoch:  194\n",
            "Epoch:  195\n",
            "Epoch:  196\n",
            "Epoch:  197\n",
            "Epoch:  198\n",
            "Epoch:  199\n",
            "time elapsed:  1938.135872586001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atrTf20E5DAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140dcbe1-c9ca-4d81-a144-535ba9c25fee"
      },
      "source": [
        "# testing the model:\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "net = ShallowNet(input_dim, n_classes)\r\n",
        "criterion = torch.nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\r\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\r\n",
        "\r\n",
        "\r\n",
        "torch.manual_seed(0)\r\n",
        "\r\n",
        "train_loader = DataLoader(Dna(subset[0][0], subset[0][1]), batch_size=10, shuffle=True)\r\n",
        "\r\n",
        "val_loader = DataLoader(val_dna, batch_size=10, shuffle=True)\r\n",
        "\r\n",
        "\r\n",
        "def train(epochs, net, criterion, optimizer, train_loader, val_loader, scheduler=None, verbose=True, device='cpu'):\r\n",
        "    net.to(device)\r\n",
        "    freq = max(epochs//15,1)\r\n",
        " \r\n",
        "    for epoch in range(1, epochs+1):\r\n",
        "        net.train()\r\n",
        "\r\n",
        "        losses_train = []\r\n",
        "        for X, target in train_loader:\r\n",
        "\r\n",
        "            X, target = X.to(device), target.to(device)\r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            x = net(X)\r\n",
        "          \r\n",
        "            train_loss = criterion(x, target)\r\n",
        "            train_loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            losses_train.append(train_loss.item())\r\n",
        "            \r\n",
        "            \r\n",
        "        if scheduler is not None:\r\n",
        "            scheduler.step()\r\n",
        "        \r\n",
        "        if verbose and epoch%freq==0:\r\n",
        "            y_pred_val =  []\r\n",
        "            y_true_val = []\r\n",
        "            net.eval()\r\n",
        "            for X, target in val_loader:\r\n",
        "                X, target = X.to(device), target.to(device)\r\n",
        "                losses_val = []  \r\n",
        "\r\n",
        "                optimizer.zero_grad()\r\n",
        "                x = net(X)\r\n",
        "                target_hat_val = torch.nn.Softmax(1)(x)\r\n",
        "\r\n",
        "                val_loss = criterion(x, target)\r\n",
        "                val_loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "                losses_val.append(val_loss.item())\r\n",
        "                                \r\n",
        "                y_pred_val.extend(target_hat_val.argmax(1).tolist())\r\n",
        "                y_true_val.extend(target.tolist())\r\n",
        "\r\n",
        "            mean_val = sum(losses_val)/len(losses_val)\r\n",
        "            mean_train = sum(losses_train)/len(losses_train)\r\n",
        "\r\n",
        "            print('Val epoch {}'.format(epoch), \\\r\n",
        "              ', Loss : {:.3}'.format(mean_train), \\\r\n",
        "              ', Accuracy on test: {:.3}'.format(accuracy_score(y_true_val, y_pred_val)) )\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgUBxcFk6gZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2683561-1a21-41b2-cd1b-6edfb67cd01f"
      },
      "source": [
        "train(200, net, criterion, optimizer, train_loader, val_loader, scheduler)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val epoch 13 , Loss : 1.1 , Accuracy on test: 0.238\n",
            "Val epoch 26 , Loss : 1.1 , Accuracy on test: 0.26\n",
            "Val epoch 39 , Loss : 1.1 , Accuracy on test: 0.268\n",
            "Val epoch 52 , Loss : 1.1 , Accuracy on test: 0.273\n",
            "Val epoch 65 , Loss : 1.1 , Accuracy on test: 0.278\n",
            "Val epoch 78 , Loss : 1.09 , Accuracy on test: 0.311\n",
            "Val epoch 91 , Loss : 1.09 , Accuracy on test: 0.368\n",
            "Val epoch 104 , Loss : 1.09 , Accuracy on test: 0.419\n",
            "Val epoch 117 , Loss : 1.09 , Accuracy on test: 0.434\n",
            "Val epoch 130 , Loss : 1.09 , Accuracy on test: 0.441\n",
            "Val epoch 143 , Loss : 1.08 , Accuracy on test: 0.466\n",
            "Val epoch 156 , Loss : 1.08 , Accuracy on test: 0.493\n",
            "Val epoch 169 , Loss : 1.07 , Accuracy on test: 0.501\n",
            "Val epoch 182 , Loss : 1.06 , Accuracy on test: 0.516\n",
            "Val epoch 195 , Loss : 1.06 , Accuracy on test: 0.522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irN5mg7vDegK"
      },
      "source": [
        "U = train_dna\r\n",
        "V = val_dna\r\n",
        "theta_prev = torch.rand(1, 300)\r\n",
        "eta = 0.05\r\n",
        "k = 3\r\n",
        "r = 30\r\n",
        "\r\n",
        "lambd = 0\r\n",
        "sel = \"stochastic_greedy\"\r\n",
        "input_dim = 180\r\n",
        "n_classes = 3\r\n",
        "\r\n",
        "\r\n",
        "random_sample = random.sample(list(U), 5)\r\n",
        "random_samp_loader = DataLoader(random_sample, batch_size=10, shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv_OuqXPDxCH",
        "outputId": "e23c5882-0a80-4312-d185-ee79f38ee700"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "net = ShallowNet(input_dim, n_classes)\r\n",
        "criterion = torch.nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\r\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\r\n",
        "train(200, net, criterion, optimizer, random_samp_loader, val_loader, scheduler)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val epoch 13 , Loss : 1.11 , Accuracy on test: 0.474\n",
            "Val epoch 26 , Loss : 1.1 , Accuracy on test: 0.493\n",
            "Val epoch 39 , Loss : 1.09 , Accuracy on test: 0.502\n",
            "Val epoch 52 , Loss : 1.09 , Accuracy on test: 0.506\n",
            "Val epoch 65 , Loss : 1.09 , Accuracy on test: 0.511\n",
            "Val epoch 78 , Loss : 1.07 , Accuracy on test: 0.516\n",
            "Val epoch 91 , Loss : 1.05 , Accuracy on test: 0.511\n",
            "Val epoch 104 , Loss : 1.04 , Accuracy on test: 0.511\n",
            "Val epoch 117 , Loss : 1.04 , Accuracy on test: 0.511\n",
            "Val epoch 130 , Loss : 1.04 , Accuracy on test: 0.511\n",
            "Val epoch 143 , Loss : 1.03 , Accuracy on test: 0.512\n",
            "Val epoch 156 , Loss : 1.02 , Accuracy on test: 0.512\n",
            "Val epoch 169 , Loss : 1.01 , Accuracy on test: 0.512\n",
            "Val epoch 182 , Loss : 0.994 , Accuracy on test: 0.512\n",
            "Val epoch 195 , Loss : 0.989 , Accuracy on test: 0.512\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}