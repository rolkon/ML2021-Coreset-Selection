{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "greedy_dss_julia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_tLuvKDficY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as torch_data\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni-NHWHtjTuj"
      },
      "source": [
        "class ShallowNet(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_class, theta=None):\n",
        "        super(ShallowNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, 100)\n",
        "        self.fc2 = torch.nn.Linear(100, num_class)\n",
        "\n",
        "        if theta != None:\n",
        "          self.fc2.weight.data = theta\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = torch.nn.ReLU()(out)\n",
        "        out = self.fc2(out)\n",
        "        return out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTPXlo19umG-"
      },
      "source": [
        "class Dat(torch_data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super(Dat, self).__init__()\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    \n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7boAr1SjUGb"
      },
      "source": [
        "def compute_grad_train(observations, theta, input_dim, n_classes):\n",
        "  '''\n",
        "  observations[0]: features, observations[1]: target\n",
        "  '''\n",
        "  feats = observations[0].reshape(1, input_dim)\n",
        "  target = observations[1].reshape(1,)\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  net = ShallowNet(input_dim, n_classes, theta.reshape(n_classes, 100))\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=0.05)\n",
        "\n",
        "  net.train()\n",
        "  optimizer.zero_grad()\n",
        "  x = net(feats)\n",
        "  train_loss = criterion(x, target)\n",
        "  train_grad = torch.autograd.grad(train_loss, net.fc2.weight, retain_graph=True)[0]\n",
        "  train_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return train_grad.reshape(1, n_classes*100), net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtI-uUrlo3L1"
      },
      "source": [
        "def compute_grad_val(net, observations, theta, input_dim, n_classes):\n",
        "  '''\n",
        "  observations[0]: features, observations[1]: target\n",
        "  '''\n",
        "  val_loader = DataLoader(observations, batch_size=40, shuffle=False)\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  net = ShallowNet(input_dim, n_classes, theta.reshape(n_classes, 100))\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=0.05)\n",
        "  # do the forward pass\n",
        "  \n",
        "  net.train()\n",
        "  for X, target in val_loader:\n",
        "    optimizer.zero_grad()\n",
        "    x = net(X)\n",
        "    val_loss = criterion(x, target)\n",
        "    val_grad = torch.autograd.grad(val_loss, net.fc2.weight, retain_graph=True)[0]\n",
        "    val_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  return val_grad.reshape(1, n_classes*100), val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NEhqHQHNwBn"
      },
      "source": [
        "def GreedyDSS(U, Val, theta_prev, eta, k, r, lambd, R, sel):\n",
        "  '''\n",
        "    Implementation of GreedyDSS (Algorithm 2) from GLISTER paper\n",
        "\n",
        "    Attributes:\n",
        "    ---\n",
        "    U: torch.tensor\n",
        "      Training data.\n",
        "    Val: torch.tensor\n",
        "      Validation data.\n",
        "    theta_0: torch.tensor\n",
        "      Model parameters initialization.\n",
        "    eta: float\n",
        "      Learning rate.\n",
        "    k: int\n",
        "      Number of point for which the model would be trained.\n",
        "    r: int\n",
        "      Number of Taylor approximations.\n",
        "    lambd: float\n",
        "      Regularization coefficient.\n",
        "    R: function\n",
        "      Regularization function.\n",
        "    sel: str\n",
        "      Selection method.\n",
        "\n",
        "    Returns\n",
        "    ---\n",
        "    S: ndarray\n",
        "      Coreset.\n",
        "  '''\n",
        "  #eps=500\n",
        "  t = 0\n",
        "  S = [U[np.random.randint(len(U))], U[np.random.randint(len(U))]]\n",
        "  theta = theta_prev\n",
        "  total_idxs = [*range(0, len(U))]\n",
        "\n",
        "  while t < r:\n",
        "    if sel == \"naive_greedy\":\n",
        "      V = Dat(U[total_idxs][0], U[total_idxs][1])\n",
        "    elif sel == \"stochastic_greedy\":\n",
        "      random_idxs = list(random.sample(total_idxs, round(len(U)*0.1)))\n",
        "      V = Dat(U[random_idxs][0], U[random_idxs][1])\n",
        "\n",
        "    g_hats = np.array([])\n",
        "\n",
        "    for e in V:\n",
        "      grad_train, net = compute_grad_train(e, theta, input_dim, n_classes)\n",
        "      theta_t_e = theta + eta *  grad_train\n",
        "      grads_s = np.array([])\n",
        "\n",
        "      for i in S:\n",
        "        grad_s, _ = compute_grad_train(i, theta_t_e, input_dim, n_classes)\n",
        "        grads_s = np.append(grads_s, grad_s)\n",
        "      grads_s = np.array(grads_s).reshape(grads_s.shape[0]//(n_classes*100), n_classes*100)\n",
        "      theta_s = theta + eta * torch.Tensor(np.sum(grads_s, axis=0))\n",
        "\n",
        "      grad_val, val_loss = compute_grad_val(net, Val, theta_s, input_dim, n_classes)\n",
        "      g_hats = np.append(g_hats, val_loss.detach().numpy() + eta * torch.matmul(grad_train, grad_val.T).detach().numpy()[0][0]) + lambd*R# * R(torch.cat(e, S))) # g hats is np.array # the largest values\n",
        "    \n",
        "    g_hats = np.array(g_hats)\n",
        "    \n",
        "    best_indices = np.argpartition(np.array(g_hats), -round(k/r))[-round(k/r):]\n",
        "\n",
        "    S.extend([V[i] for i in best_indices])\n",
        "\n",
        "    if sel == \"stochastic_greedy\":\n",
        "      subset_idxs = list(np.array(random_idxs)[best_indices])\n",
        "\n",
        "    else:\n",
        "      subset_idxs = list(np.array(total_idxs)[best_indices])\n",
        "\n",
        "    total_idxs = list(set(total_idxs).difference(set(subset_idxs)))\n",
        "\n",
        "    grads_theta = torch.zeros(n_classes*100).reshape(1, n_classes*100)\n",
        "\n",
        "    for elem in [V[i] for i in best_indices]:\n",
        "      grad, _ = compute_grad_train(elem, theta, input_dim, n_classes)\n",
        "\n",
        "      grads_theta += grad\n",
        "\n",
        "    theta = theta + grads_theta\n",
        "\n",
        "    t += 1\n",
        "\n",
        "  return S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYE6VdlGuqaW"
      },
      "source": [
        "def glister_online(U, Val, S_0, k, theta_prev, eta, T, L, r, lambd, R, sel):\n",
        "  '''\n",
        "  Attributes:\n",
        "  ---\n",
        "  U: torch.tensor\n",
        "    Training data.\n",
        "  Val: torch.tensor\n",
        "    Validation data.\n",
        "  S_0: torch.tensor\n",
        "    Initial subset.\n",
        "  k: int\n",
        "    Size of the initial subset.\n",
        "  theta_prev: torch.tensor\n",
        "    Model parameter initialization.\n",
        "  eta: float\n",
        "    Learning rate.\n",
        "  T: int\n",
        "    Total epochs.\n",
        "  L: int\n",
        "    Epoch interval for selection.\n",
        "  r: int\n",
        "    Number of Taylor approximations.\n",
        "  lambd: float\n",
        "    Regularization coefficient.\n",
        "  R: function\n",
        "    Regularization function.\n",
        "  sel: str\n",
        "    Selection Method.\n",
        "  \n",
        "  Returns:\n",
        "  ---\n",
        "  S_T: torch.tensor\n",
        "    Final subset\n",
        "  theta_T: torch.tensor\n",
        "    Parameters.\n",
        "  '''\n",
        "  theta = theta_prev\n",
        "  S_t = S_0\n",
        "\n",
        "  for t in range(T):\n",
        "    if t % L == 0:\n",
        "      print(\"Progress: {t}/{T}\".format(t=t, T=T))\n",
        "      S_t = GreedyDSS(U=U, Val=Val, theta_prev=theta, eta=eta, k=k, r=r, lambd=0, R=0, sel=sel)\n",
        "\n",
        "    model = ShallowNet(input_dim, n_classes)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    loader = DataLoader(S_t, batch_size=20, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    for X, y in loader:\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      x = model(X)\n",
        "      train_loss = criterion(x, y)\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    theta = model.fc2.weight.reshape(1, n_classes*100)\n",
        "\n",
        "  return S_t, model.fc2.weight\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpbIu12fzpqE"
      },
      "source": [
        "def train(epochs, net, criterion, optimizer, train_loader, val_loader, verbose=True, device='cpu'):\n",
        "    net.to(device)\n",
        "    freq = max(epochs//15,1)\n",
        " \n",
        "    for epoch in range(1, epochs+1):\n",
        "        net.train()\n",
        "\n",
        "        losses_train = []\n",
        "        for X, target in train_loader:\n",
        "\n",
        "            #X, target = X.to(device), target.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            x = net(X)\n",
        "          \n",
        "            train_loss = criterion(x, target)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            losses_train.append(train_loss.item())\n",
        "\n",
        "        if verbose and epoch%freq==0:\n",
        "            y_pred_val =  []\n",
        "            y_true_val = []\n",
        "            net.eval()\n",
        "            for X, target in val_loader:\n",
        "                #X, target = X.to(device), target.to(device)\n",
        "                losses_val = []  \n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                x = net(X)\n",
        "                target_hat_val = torch.nn.Softmax(1)(x)\n",
        "\n",
        "                val_loss = criterion(x, target)\n",
        "                losses_val.append(val_loss.item())\n",
        "                                \n",
        "                y_pred_val.extend(target_hat_val.argmax(1).tolist())\n",
        "                y_true_val.extend(target.tolist())\n",
        "\n",
        "            mean_val = sum(losses_val)/len(losses_val)\n",
        "            mean_train = sum(losses_train)/len(losses_train)\n",
        "            \n",
        "            print('Val epoch {}'.format(epoch), ', Loss : {:.3}'.format(mean_train), ', Accuracy on test: {:.3}'.format(accuracy_score(y_true_val, y_pred_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItgRUWgPzBnR"
      },
      "source": [
        "def experimental_setting(train_data, val_data, test_data, theta_prev, eta, k, r, lambd, R, sel, input_dim, n_classes, T, L, batch_size, network):\n",
        "\n",
        "  print(\"Glister online:\")\n",
        "  start = time.perf_counter()\n",
        "  subset = glister_online(U=train_data, Val=val_data, theta_prev=theta_prev, S_0 = train_data[[np.random.randint(len(train_data)), np.random.randint(len(train_data))]], eta=eta, k=k, r=r, lambd=lambd, R=R, sel=sel, T=T, L=L)\n",
        "  print(\"time elapsed: \", time.perf_counter()-start)\n",
        "  print(\"Testing the model:\")\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  net = network\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=0.05)\n",
        "\n",
        "  train_loader = DataLoader(subset[0], batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(val_data, batch_size=batch_size*2, shuffle=False)\n",
        "  test_loader = DataLoader(test_data, batch_size=batch_size*2, shuffle=False)\n",
        "\n",
        "  train(200, net, criterion, optimizer, train_loader, test_loader, verbose=True)\n",
        "\n",
        "  print(\"Random comparison:\")\n",
        "\n",
        "  random_sample = random.sample(list(train_data), k)\n",
        "  random_samp_loader = DataLoader(random_sample, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  net_2 = ShallowNet(input_dim, n_classes)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(net_2.parameters(), lr=0.05)\n",
        "  train(200, net_2, criterion, optimizer, random_samp_loader, test_loader, verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2nQucCGCBc6"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "digits, targets = load_digits(return_X_y=True)\n",
        "digits = digits.astype(np.float32) / 255   # scaling\n",
        "\n",
        "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
        "digits_tr, digits_val, targets_tr, targets_val = train_test_split(digits_train, targets_train, random_state=0, shuffle=False)\n",
        "\n",
        "input_dim = 8*8\n",
        "n_classes = 10\n",
        "\n",
        "train_digits = Dat(digits_tr, targets_tr)\n",
        "val_digits = Dat(digits_val, targets_val)\n",
        "test_digits = Dat(digits_test, targets_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_InjPgieCLEA",
        "outputId": "4c89ae8d-98c7-4026-d3e6-ee9c1830a120"
      },
      "source": [
        "# ---------- SKLEARN DIGITS data set ------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "n_classes = 10\n",
        "input_dim = 64\n",
        "#print(\"Approximately 10% of data\")\n",
        "#experimental_setting(train_digits, val_digits, test_digits, torch.randn(1, 1000), 0.05, 100, 50, 0, 0, \"naive_greedy\", 64, 10, 200, 20, 20, ShallowNet(64, 10)) # ~10% of data, no regularization\n",
        "\n",
        "print(\"Approximately 30% of data\")\n",
        "experimental_setting(train_digits, val_digits, test_digits, torch.randn(1, 1000), 0.05, 300, 9, 0, 0, \"naive_greedy\", 64, 10, 200, 20, 20, ShallowNet(64, 10)) # ~30% of data, no regularization\n",
        "\n",
        "print(\"Approximately 50% of data\")\n",
        "experimental_setting(train_digits, val_digits, test_digits, torch.randn(1, 1000), 0.05, 500, 15, 0, 0, \"naive_greedy\", 64, 10, 200, 20, 20, ShallowNet(64, 10)) # ~50% of data, no regularization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Approximately 30% of data\n",
            "Glister online:\n",
            "Progress: 0/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Progress: 20/200\n",
            "Progress: 40/200\n",
            "Progress: 60/200\n",
            "Progress: 80/200\n",
            "Progress: 100/200\n",
            "Progress: 120/200\n",
            "Progress: 140/200\n",
            "Progress: 160/200\n",
            "Progress: 180/200\n",
            "time elapsed:  14482.551718878001\n",
            "Testing the model:\n",
            "Val epoch 13 , Loss : 2.29 , Accuracy on test: 0.0956\n",
            "Val epoch 26 , Loss : 2.29 , Accuracy on test: 0.0956\n",
            "Val epoch 39 , Loss : 2.28 , Accuracy on test: 0.0956\n",
            "Val epoch 52 , Loss : 2.28 , Accuracy on test: 0.0956\n",
            "Val epoch 65 , Loss : 2.27 , Accuracy on test: 0.0956\n",
            "Val epoch 78 , Loss : 2.26 , Accuracy on test: 0.0956\n",
            "Val epoch 91 , Loss : 2.25 , Accuracy on test: 0.0956\n",
            "Val epoch 104 , Loss : 2.24 , Accuracy on test: 0.0978\n",
            "Val epoch 117 , Loss : 2.23 , Accuracy on test: 0.107\n",
            "Val epoch 130 , Loss : 2.21 , Accuracy on test: 0.129\n",
            "Val epoch 143 , Loss : 2.19 , Accuracy on test: 0.167\n",
            "Val epoch 156 , Loss : 2.16 , Accuracy on test: 0.209\n",
            "Val epoch 169 , Loss : 2.13 , Accuracy on test: 0.298\n",
            "Val epoch 182 , Loss : 2.08 , Accuracy on test: 0.358\n",
            "Val epoch 195 , Loss : 2.03 , Accuracy on test: 0.418\n",
            "Random comparison:\n",
            "Val epoch 13 , Loss : 2.29 , Accuracy on test: 0.1\n",
            "Val epoch 26 , Loss : 2.29 , Accuracy on test: 0.1\n",
            "Val epoch 39 , Loss : 2.28 , Accuracy on test: 0.1\n",
            "Val epoch 52 , Loss : 2.28 , Accuracy on test: 0.1\n",
            "Val epoch 65 , Loss : 2.27 , Accuracy on test: 0.102\n",
            "Val epoch 78 , Loss : 2.27 , Accuracy on test: 0.116\n",
            "Val epoch 91 , Loss : 2.26 , Accuracy on test: 0.153\n",
            "Val epoch 104 , Loss : 2.25 , Accuracy on test: 0.167\n",
            "Val epoch 117 , Loss : 2.24 , Accuracy on test: 0.18\n",
            "Val epoch 130 , Loss : 2.22 , Accuracy on test: 0.184\n",
            "Val epoch 143 , Loss : 2.2 , Accuracy on test: 0.189\n",
            "Val epoch 156 , Loss : 2.17 , Accuracy on test: 0.271\n",
            "Val epoch 169 , Loss : 2.14 , Accuracy on test: 0.322\n",
            "Val epoch 182 , Loss : 2.1 , Accuracy on test: 0.36\n",
            "Val epoch 195 , Loss : 2.05 , Accuracy on test: 0.404\n",
            "Approximately 50% of data\n",
            "Glister online:\n",
            "Progress: 0/200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdgrYJGz9ed3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVAA1AHq8NdO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}