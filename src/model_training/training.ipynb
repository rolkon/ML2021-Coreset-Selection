{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this notebook, the selected models are trained on the datasets generated by the coreset selectors.\n",
    "\n",
    "### Models\n",
    "\n",
    "- Resnet18\n",
    "- Mobilenet\n",
    "- Inception\n",
    "- VGGnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../submodules/PyTorch_CIFAR10/cifar10_models/')\n",
    "sys.path.append('../submodules/PyTorch_CIFAR10')\n",
    "\n",
    "from resnet import resnet18\n",
    "from mobilenetv2 import mobilenet_v2\n",
    "from inception import inception_v3\n",
    "from vgg import vgg11_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Epoch Function\n",
    "\n",
    "Function that trains and evaluates the model for a given number of epochs.\n",
    "Switches from optimizer1 to optimizer2 after 10 epochs, for using SWAT technique\n",
    "\n",
    "[https://arxiv.org/pdf/1712.07628.pdf](https://arxiv.org/pdf/1712.07628.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "def train_model(epochs, net, net_name, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, device='cuda'):\n",
    "    freq = max(epochs//20,1)\n",
    " \n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "\n",
    "        losses_train = []\n",
    "        for X, target in train_loader:\n",
    "            X, target = X.to(device), target.to(device)\n",
    "            \n",
    "            net_output = net(X)\n",
    "            loss = criterion(net_output, target)\n",
    "            losses_train.append(float(loss))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #scheduler is cosine annealing, so its called in the step loop\n",
    "            if scheduler != None:\n",
    "                scheduler.step()\n",
    "        \n",
    "        if verbose and epoch%freq==0:\n",
    "\n",
    "            y_pred_val =  []\n",
    "            y_true_val = []\n",
    "\n",
    "            net.eval()\n",
    "\n",
    "            losses_val = []\n",
    "            for X, target in val_loader:\n",
    "                X, target = X.to(device), target.to(device)\n",
    "\n",
    "                # Compute the validation loss\n",
    "                target_hat_val = net(X)\n",
    "\n",
    "                loss = criterion(target_hat_val, target)\n",
    "                losses_val.append(float(loss))\n",
    "\n",
    "                y_pred_val.extend(target_hat_val.argmax(1).tolist())\n",
    "                y_true_val.extend(target.tolist())\n",
    "\n",
    "            mean_val = sum(losses_val)/len(losses_val)\n",
    "            mean_train = sum(losses_train)/len(losses_train)\n",
    "\n",
    "            print('Timestamp: ', datetime.now().strftime(\"%H:%M:%S\"), \\\n",
    "                  '\\tVal epoch {}'.format(epoch), \\\n",
    "                  '\\n\\tModel: {}'.format(net_name), \\\n",
    "                  '\\n\\tLoss Train: {:.3}'.format(mean_train), \\\n",
    "                  ',\\n\\tLoss Test: {:.3}'.format(mean_val),\\\n",
    "                  ',\\n\\tAccuracy on test: {:.3}'.format(accuracy_score(y_true_val, y_pred_val)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evaluation Function\n",
    "\n",
    "Used to evaluate model after training. Function puts the result in a log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "def eval_model(net, net_name, coreset_selector_name, percentage_of_dataset, criterion, test_loader, logfile):\n",
    "    y_pred_val =  []\n",
    "    y_true_val = []\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    losses_val = []\n",
    "    for X, target in val_loader:\n",
    "        X, target = X.to(device), target.to(device)\n",
    "\n",
    "        # Compute the validation loss\n",
    "        target_hat_val = net(X)\n",
    "\n",
    "        loss = criterion(target_hat_val, target)\n",
    "        losses_val.append(float(loss))\n",
    "\n",
    "        y_pred_val.extend(target_hat_val.argmax(1).tolist())\n",
    "        y_true_val.extend(target.tolist())\n",
    "\n",
    "    mean_val = sum(losses_val)/len(losses_val)\n",
    "    \n",
    "    logtext = '\\nTimestamp: ' + datetime.now().strftime(\"%H:%M:%S\") + \\\n",
    "            '\\n\\tModel: {}'.format(net_name) + \\\n",
    "            '\\n\\tCoreset Selector: {}'.format(coreset_selector_name) + \\\n",
    "            '\\n\\tPercentage of Dataset: {}'.format(percentage_of_dataset) + \\\n",
    "            '\\n\\tLoss Test: {:.3}'.format(mean_val) + \\\n",
    "            '\\n\\tAccuracy on test: {:.3}'.format(accuracy_score(y_true_val, y_pred_val))\n",
    "    \n",
    "    f = open(logfile, 'a')\n",
    "    f.write(logtext)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#factors selected from torch docs\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                            train=False, \n",
    "                                            transform=transform,\n",
    "                                            download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreset Training Function\n",
    "\n",
    "Function that trains the model on selected datapoints from the dataset. The argument is given as a list of indices.\n",
    "\n",
    "Additionally, logs the last trainings and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from schduler import WarmupCosineLR\n",
    "\n",
    "def train_and_save_models(train_indices, percentage_of_dataset):\n",
    "    \n",
    "    train_datasubset = [train_dataset[i] for i in train_indices]\n",
    "    \n",
    "    # Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_datasubset,\n",
    "                                               batch_size=128, \n",
    "                                               shuffle=True,\n",
    "                                               drop_last=True\n",
    "                                              )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=128, \n",
    "                                              shuffle=False,\n",
    "                                              drop_last=True\n",
    "                                             )\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Training parameters\n",
    "    num_epochs = 100\n",
    "    learning_rate = 1e-2\n",
    "    weight_decay = 1e-3\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "\n",
    "    # Models\n",
    "    model_resnet = resnet18()\n",
    "    model_mobilenet = mobilenet_v2()\n",
    "    model_inception = inception_v3()\n",
    "    model_vgg = vgg11_bn()\n",
    "\n",
    "    models = [model_resnet, model_mobilenet, model_inception, model_vgg]\n",
    "    model_labels = ['resnet', 'mobilenet', 'inception', 'vgg']\n",
    "    \n",
    "    #train selected models on subset\n",
    "    for model, label in zip(models, model_labels):\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate\n",
    "            weight_decay=weight_decay,\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=1, gamma=1/10)\n",
    "        \n",
    "        #'''\n",
    "        # Scheduler\n",
    "        scheduler = WarmupCosineLR(\n",
    "            optimizer,\n",
    "            warmup_epochs=total_steps * 0.1,\n",
    "            max_epochs = total_steps)\n",
    "        #'''\n",
    "\n",
    "        # Train the model\n",
    "        train_model(num_epochs, model, label, criterion, optimizer, train_loader, test_loader, scheduler=scheduler, verbose=True, device=device)\n",
    "\n",
    "        # Save the model checkpoint\n",
    "        torch.save(model.state_dict(), 'model_weights/{:03}_{}.pt'.format(percentage_of_dataset, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/opt/python-3.6.8/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp:  20:33:28 \tVal epoch 4 \n",
      "\tModel: resnet \n",
      "\tLoss Train: 0.518 ,\n",
      "\tLoss Test: 0.651 ,\n",
      "\tAccuracy on test: 0.777\n",
      "Timestamp:  20:35:40 \tVal epoch 8 \n",
      "\tModel: resnet \n",
      "\tLoss Train: 0.441 ,\n",
      "\tLoss Test: 0.651 ,\n",
      "\tAccuracy on test: 0.777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5d53c11971e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_and_save_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-afbce73922fb>\u001b[0m in \u001b[0;36mtrain_and_save_models\u001b[0;34m(train_indices, percentage_of_dataset)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Save the model checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a24c795772c0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, net, net_name, criterion, optimizer1, optimizer2, train_loader, val_loader, scheduler, verbose, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlosses_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mnet_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_indices = np.arange(0, len(train_dataset))\n",
    "\n",
    "train_and_save_models(train_indices, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
