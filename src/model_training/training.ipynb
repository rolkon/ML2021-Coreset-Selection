{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this notebook, the selected models are trained on the datasets generated by the coreset selectors.\n",
    "\n",
    "### Models\n",
    "\n",
    "- Resnet18\n",
    "- Mobilenet\n",
    "- Inception\n",
    "- VGGnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Epoch Function\n",
    "\n",
    "Function that trains and evaluates the model for a given number of epochs.\n",
    "Switches from optimizer1 to optimizer2 after 10 epochs, for using SWAT technique\n",
    "\n",
    "[https://arxiv.org/pdf/1712.07628.pdf](https://arxiv.org/pdf/1712.07628.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "def train_model(epochs, net, net_name, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, device='cuda'):\n",
    "    freq = max(epochs//20,1)\n",
    "    \n",
    "    accuracies = []\n",
    " \n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "\n",
    "        losses_train = []\n",
    "        for X, target in train_loader:\n",
    "            X, target = X.to(device), target.to(device)\n",
    "            \n",
    "            net_output = net(X)\n",
    "            loss = criterion(net_output, target)\n",
    "            losses_train.append(float(loss))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #scheduler is cosine annealing, so its called in the step loop\n",
    "            if scheduler != None:\n",
    "                scheduler.step()\n",
    "        \n",
    "        if verbose and epoch%freq==0:\n",
    "\n",
    "            y_pred_val =  []\n",
    "            y_true_val = []\n",
    "\n",
    "            net.eval()\n",
    "\n",
    "            losses_val = []\n",
    "            for X, target in val_loader:\n",
    "                X, target = X.to(device), target.to(device)\n",
    "\n",
    "                # Compute the validation loss\n",
    "                target_hat_val = net(X)\n",
    "\n",
    "                loss = criterion(target_hat_val, target)\n",
    "                losses_val.append(float(loss))\n",
    "\n",
    "                y_pred_val.extend(target_hat_val.argmax(1).tolist())\n",
    "                y_true_val.extend(target.tolist())\n",
    "\n",
    "            mean_val = sum(losses_val)/len(losses_val)\n",
    "            mean_train = sum(losses_train)/len(losses_train)\n",
    "            \n",
    "            accuracies.append(accuracy_score(y_true_val, y_pred_val))\n",
    "            \n",
    "            print('Timestamp: ', datetime.now().strftime(\"%H:%M:%S\"), \\\n",
    "                  '\\tVal epoch {}'.format(epoch), \\\n",
    "                  '\\n\\tModel: {}'.format(net_name), \\\n",
    "                  '\\n\\tLoss Train: {:.3}'.format(mean_train), \\\n",
    "                  ',\\n\\tLoss Test: {:.3}'.format(mean_val),\\\n",
    "                  ',\\n\\tAccuracy on test: {:.3}'.format(accuracy_score(y_true_val, y_pred_val)) )\n",
    "            \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evaluation Function\n",
    "\n",
    "Used to evaluate model after training. Function puts the result in a log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "def eval_model(net, net_name, coreset_selector_name, percentage_of_dataset, criterion, test_loader, logfile):\n",
    "    y_pred_val =  []\n",
    "    y_true_val = []\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    losses_val = []\n",
    "    for X, target in val_loader:\n",
    "        X, target = X.to(device), target.to(device)\n",
    "\n",
    "        # Compute the validation loss\n",
    "        target_hat_val = net(X)\n",
    "\n",
    "        loss = criterion(target_hat_val, target)\n",
    "        losses_val.append(float(loss))\n",
    "\n",
    "        y_pred_val.extend(target_hat_val.argmax(1).tolist())\n",
    "        y_true_val.extend(target.tolist())\n",
    "\n",
    "    mean_val = sum(losses_val)/len(losses_val)\n",
    "    \n",
    "    logtext = '\\nTimestamp: ' + datetime.now().strftime(\"%H:%M:%S\") + \\\n",
    "            '\\n\\tModel: {}'.format(net_name) + \\\n",
    "            '\\n\\tCoreset Selector: {}'.format(coreset_selector_name) + \\\n",
    "            '\\n\\tPercentage of Dataset: {}'.format(percentage_of_dataset) + \\\n",
    "            '\\n\\tLoss Test: {:.3}'.format(mean_val) + \\\n",
    "            '\\n\\tAccuracy on test: {:.3}'.format(accuracy_score(y_true_val, y_pred_val))\n",
    "    \n",
    "    f = open(logfile, 'a')\n",
    "    f.write(logtext)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#factors selected from torch docs\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                            train=False, \n",
    "                                            transform=transform,\n",
    "                                            download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreset Training Function\n",
    "\n",
    "Function that trains the model on selected datapoints from the dataset. The argument is given as a list of indices, to make it suitable for core-set selection techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../submodules/PyTorch_CIFAR10')\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from schduler import WarmupCosineLR\n",
    "\n",
    "def train_and_save_models(models, model_names, train_indices, selector_name=None):\n",
    "    \n",
    "    train_datasubset = [train_dataset[i] for i in train_indices]\n",
    "    \n",
    "    # Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_datasubset,\n",
    "                                               batch_size=128, \n",
    "                                               shuffle=True,\n",
    "                                               drop_last=True\n",
    "                                              )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=128, \n",
    "                                              shuffle=False,\n",
    "                                              drop_last=True\n",
    "                                             )\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs = 100\n",
    "    learning_rate = 1e-2\n",
    "    weight_decay = 1e-2\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "    \n",
    "    #train selected models on subset\n",
    "    for model, label in zip(models, model_names):\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = WarmupCosineLR(\n",
    "            optimizer,\n",
    "            warmup_epochs=total_steps * 0.3,\n",
    "            max_epochs = total_steps)\n",
    "\n",
    "        # Train the model\n",
    "        accuracies = train_model(num_epochs, model, label, criterion, optimizer, train_loader, test_loader, scheduler=scheduler, verbose=True, device=device)\n",
    "\n",
    "        # Save the model weights\n",
    "        percentage_of_dataset = np.round((len(train_datasubset)/len(train_dataset))*100).astype(int)\n",
    "        \n",
    "        if selector_name == None:\n",
    "            weights_filename = 'model_weights/{:03}_{}.pt'.format(percentage_of_dataset, label)\n",
    "            results_filename = 'accuracy_results/{:03}_{}.csv'.format(percentage_of_dataset, label)\n",
    "        else:\n",
    "            weights_filename = 'model_weights/{:03}_{}_{}.pt'.format(percentage_of_dataset, label, selector_name)\n",
    "            results_filename = 'accuracy_results/{:03}_{}_{}.csv'.format(percentage_of_dataset, label, selector_name)\n",
    "        torch.save(model.state_dict(), weights_filename)\n",
    "        \n",
    "        np.savetxt(results_filename, accuracies, delimiter=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../submodules/PyTorch_CIFAR10/cifar10_models/')\n",
    "\n",
    "from resnet import resnet18\n",
    "from mobilenetv2 import mobilenet_v2\n",
    "from densenet import densenet121\n",
    "from vgg import vgg11_bn\n",
    "\n",
    "# Models\n",
    "model_resnet = resnet18()\n",
    "model_mobilenet = mobilenet_v2()\n",
    "model_vgg = vgg11_bn()\n",
    "model_densenet = densenet121()\n",
    "\n",
    "models = [model_resnet, model_mobilenet, model_vgg, model_densenet]\n",
    "model_names = ['resnet', 'mobilenet', 'vgg', 'densenet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on dataset\n",
    "\n",
    "use np seed 0 to fix selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "train_indices = np.random.permutation(np.arange(0, len(train_dataset)))[:20000]\n",
    "\n",
    "train_and_save_models(models, model_names, train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for data subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append('../greedy_k_centers/')\n",
    "\n",
    "indices_k_center = np.loadtxt('../greedy_k_centers/k_centers_indices.csv', delimiter=',')\n",
    "\n",
    "# 10% of dataset\n",
    "train_indices = np.array(indices_k_center[:5000]).astype(int)\n",
    "\n",
    "train_and_save_models(models, model_names, train_indices, selector_name='k_centers')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
